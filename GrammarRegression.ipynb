{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic grammar validation\n",
    "\n",
    "Training a neural net to determine whether a given sentence is gramatically valid or not\n",
    "\n",
    "The plan is\n",
    "1. Take sentences\n",
    "2. Randomly perturb them\n",
    "3. Train a classifier to distinguish perturbed sentences from the original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "\n",
    "with open('texts/alg.txt') as f:\n",
    "    text += f.read()\n",
    "    \n",
    "text += '\\n\\n'\n",
    "\n",
    "with open('texts/testament.txt') as f:\n",
    "    text += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for sent in sent_detector.tokenize(text):\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    if tokens[0].isdigit():\n",
    "        tokens = tokens[1:]\n",
    "    sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Как',\n",
       " 'ваш',\n",
       " 'маленький',\n",
       " 'GPS',\n",
       " 'в',\n",
       " 'считанные',\n",
       " 'секунды',\n",
       " 'на',\n",
       " '\\xad',\n",
       " 'ходит',\n",
       " 'самый',\n",
       " 'быстрый',\n",
       " 'пуrь',\n",
       " 'из',\n",
       " 'несметного',\n",
       " 'множества',\n",
       " 'возможных',\n",
       " 'маршруrов',\n",
       " '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Как', 'CONJ'),\n",
       "  ('ваш', 'A-PRO=m'),\n",
       "  ('маленький', 'A=m'),\n",
       "  ('GPS', 'NONLEX'),\n",
       "  ('в', 'PR'),\n",
       "  ('считанные', 'V'),\n",
       "  ('секунды', 'S'),\n",
       "  ('на', 'PR'),\n",
       "  ('\\xad', 'S'),\n",
       "  ('ходит', 'V'),\n",
       "  ('самый', 'A-PRO=m'),\n",
       "  ('быстрый', 'A=m'),\n",
       "  ('пуrь', 'S'),\n",
       "  ('из', 'PR'),\n",
       "  ('несметного', 'A=n'),\n",
       "  ('множества', 'S'),\n",
       "  ('возможных', 'A=pl'),\n",
       "  ('маршруrов', 'S'),\n",
       "  ('?', 'NONLEX')],\n",
       " [('Когда', 'CONJ'),\n",
       "  ('вы', 'S-PRO'),\n",
       "  ('покупаете', 'V'),\n",
       "  ('что-то', 'S-PRO'),\n",
       "  ('в', 'PR'),\n",
       "  ('Интернете', 'S'),\n",
       "  (',', 'NONLEX'),\n",
       "  ('как', 'CONJ'),\n",
       "  ('обеспечивается', 'V'),\n",
       "  ('защита', 'S'),\n",
       "  ('номера', 'S'),\n",
       "  ('вашей', 'A-PRO=f'),\n",
       "  ('кредитной', 'A=f'),\n",
       "  ('карты', 'S'),\n",
       "  ('от', 'PR'),\n",
       "  ('перехвата', 'S'),\n",
       "  ('злоумышленником', 'S'),\n",
       "  ('?', 'NONLEX')]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag_sents(sentences[2:4], lang='rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(sentence):\n",
    "    return np.random.permutation(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slightly_perturb(sentence):\n",
    "    remove_from = np.random.randint(len(sentence))\n",
    "    insert_to = np.random.randint(len(sentence))\n",
    "    \n",
    "    token = sentence[remove_from]\n",
    "    sentence = sentence[:remove_from] + sentence[remove_from+1:]\n",
    "    sentence = sentence[:insert_to] + [token] + sentence[insert_to:]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_sentences = [perturb(sent) if np.random.rand() > 0.5 else slightly_perturb(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "bpe_model_location = 'ru.wiki.bpe.op1000.model'\n",
    "bpe_vec_location = 'ru.wiki.bpe.op1000.d25.w2v.bin'\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(bpe_model_location)\n",
    "bpe_model = KeyedVectors.load_word2vec_format(bpe_vec_location, binary=True)\n",
    "\n",
    "def bpe_embed(text):\n",
    "    pieces = sp.encode_as_pieces(text)\n",
    "    embedding = np.zeros(bpe_model.vector_size)\n",
    "    piece_count = 0\n",
    "\n",
    "    for binary_piece in pieces:\n",
    "        piece = binary_piece.decode('utf-8')\n",
    "        try:\n",
    "            embedding += bpe_model[piece]\n",
    "            piece_count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if piece_count:\n",
    "        embedding /= piece_count\n",
    "            \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = nltk.tag._get_tagger('rus')\n",
    "pos_tags = list(tagger.classes)\n",
    "\n",
    "def embed_sentences(sentences):\n",
    "    for sentence in tagger.tag_sents(sentences):\n",
    "        sent_embedding = []\n",
    "    \n",
    "        for token, pos in sentence:\n",
    "            token_embedding = bpe_embed(token)\n",
    "            pos_embedding = np.zeros(len(pos_tags))\n",
    "            pos_embedding[pos_tags.index(pos)] = 1\n",
    "            sent_embedding.append(np.concatenate([token_embedding, pos_embedding]))\n",
    "        \n",
    "        yield sent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(embed_sentences(sentences + perturbed_sentences))\n",
    "y = [1 for sentence in sentences] + [0 for sentence in perturbed_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "normal_dist = torch.distributions.Normal(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lstm = nn.LSTM(136, 32, batch_first=True)\n",
    "classifier = nn.Sequential(nn.Linear(64, 64), nn.Tanh(), nn.Linear(64, 64), nn.Tanh(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "opt = torch.optim.Adam(itertools.chain(lstm.parameters(), classifier.parameters()))\n",
    "def validate_grammar(sentences):\n",
    "    c0, h0 = normal_dist.sample((1, 1, 32)), normal_dist.sample((1,1,32))\n",
    "    _, (cn, hn) = lstm(torch.Tensor(sentences), (c0, h0))\n",
    "    return classifier(torch.cat((cn[0], hn[0]), dim=1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch():\n",
    "    for sentence, validity in zip(X_train, y_train):\n",
    "        pred = validate_grammar([sentence])\n",
    "        loss = F.binary_cross_entropy(pred, torch.Tensor([validity]))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test():\n",
    "    print(f'train accuracy {accuracy_score(y_train, [validate_grammar([sentence]) for sentence in X_train])}')\n",
    "    print(f'test accuracy  {accuracy_score(y_test, [validate_grammar([sentence]) for sentence in X_test])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_epoch()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
